# train.py ä¼˜åŒ–æ€»ç»“

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

1. é€‚é… 64Ã—64 å›¾åƒå°ºå¯¸
2. æ”¯æŒçº¯æ–‡æœ¬é¢„è®­ç»ƒï¼ˆç§»é™¤é—®ç­”å¯¹æ ¼å¼ï¼‰
3. æ”¯æŒ padding mask
4. ä¼˜åŒ–è®­ç»ƒæ€§èƒ½ï¼ˆæ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰ï¼‰

---

## âœ… å·²å®Œæˆçš„ä¼˜åŒ–

### 1. å›¾åƒå°ºå¯¸é€‚é…

**ä¿®æ”¹**ï¼š
- âœ… é»˜è®¤ `img_size` ä» `256` æ”¹ä¸º `64`
- âœ… æ‰€æœ‰ç›¸å…³ä»£ç å·²æ›´æ–°

**å½±å“**ï¼š
- è®­ç»ƒé€Ÿåº¦æå‡çº¦ 16 å€
- æ˜¾å­˜å ç”¨å‡å°‘çº¦ 16 å€

### 2. æ•°æ®æ ¼å¼é€‚é…

**ä¿®æ”¹**ï¼š
- âœ… ç§»é™¤ `question_size` å’Œ `max_answer_tokens` å‚æ•°
- âœ… ç§»é™¤ `RobustEncodingConfig` ä¾èµ–
- âœ… ä½¿ç”¨ `use_chat_template` å’Œ `max_tokens` å‚æ•°
- âœ… é€‚é…çº¯æ–‡æœ¬é¢„è®­ç»ƒæ ¼å¼

**ä¹‹å‰**ï¼š
```python
dataset = TokenImageDataset(
    question_size=(32, 32),
    token_encoder_config=config,
    max_answer_tokens=...,
)
```

**ç°åœ¨**ï¼š
```python
dataset = TokenImageDataset(
    img_size=64,
    use_chat_template=False,  # é¢„è®­ç»ƒ
    max_tokens=None,  # ä½¿ç”¨å›¾åƒå®¹é‡
)
```

### 3. è®­ç»ƒå¾ªç¯ä¼˜åŒ–

**ä¿®æ”¹**ï¼š
- âœ… ç§»é™¤ `question` å’Œ `noisy` çš„ä½¿ç”¨
- âœ… ä½¿ç”¨ `clean` å’Œ `mask`ï¼ˆä» dataset è¿”å›ï¼‰
- âœ… æ”¯æŒæ— æ¡ä»¶è®­ç»ƒï¼ˆ`condition=None`ï¼‰
- âœ… æ”¯æŒ padding mask

**ä¹‹å‰**ï¼š
```python
noisy = batch['noisy']
question = batch['question']
clean_pred = model(noisy, t, condition=question_patches)
loss = mse_loss(clean_pred_img, clean)
```

**ç°åœ¨**ï¼š
```python
clean = batch['clean']
mask = batch['mask']
noisy_target, _ = add_noise_to_timestep(clean, t)
clean_pred = model(noisy_target, t, condition=None, mask=mask)
# Masked loss
mask_expanded = mask.unsqueeze(1)
diff = (clean_pred_img - clean) ** 2
masked_diff = diff * mask_expanded
loss = masked_diff.sum() / (mask_expanded.sum() + 1e-8)
```

### 4. æ··åˆç²¾åº¦è®­ç»ƒ

**åŠŸèƒ½**ï¼š
- âœ… ä½¿ç”¨ `autocast` å’Œ `GradScaler`
- âœ… æ”¯æŒé€šè¿‡ `--use_amp` å¼€å…³
- âœ… é»˜è®¤å¯ç”¨

**ä¼˜åŠ¿**ï¼š
- è®­ç»ƒé€Ÿåº¦æå‡çº¦ 2 å€
- æ˜¾å­˜å ç”¨å‡å°‘çº¦ 50%

### 5. æ¢¯åº¦ç´¯ç§¯

**åŠŸèƒ½**ï¼š
- âœ… æ”¯æŒ `gradient_accumulation_steps` å‚æ•°
- âœ… åœ¨è®­ç»ƒå¾ªç¯ä¸­æ­£ç¡®å¤„ç†æ¢¯åº¦ç´¯ç§¯
- âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨è€ƒè™‘æ¢¯åº¦ç´¯ç§¯

**ä¼˜åŠ¿**ï¼š
- å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æœ‰æ•ˆ batch size
- åœ¨æ˜¾å­˜å—é™æ—¶ä»èƒ½è®­ç»ƒ

### 6. å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–

**ä¿®æ”¹**ï¼š
- âœ… è€ƒè™‘æ¢¯åº¦ç´¯ç§¯çš„å®é™…ä¼˜åŒ–æ­¥æ•°
- âœ… Warmup + Cosine Decay
- âœ… æ¯ä¸ªä¼˜åŒ–æ­¥éª¤åæ›´æ–°ï¼ˆè€Œä¸æ˜¯æ¯ä¸ª epochï¼‰

**ä¹‹å‰**ï¼š
```python
total_steps = len(dataloader) * epochs
scheduler.step()  # æ¯ä¸ª epoch æ›´æ–°ä¸€æ¬¡
```

**ç°åœ¨**ï¼š
```python
effective_batches = len(dataloader) // gradient_accumulation_steps
total_steps = effective_batches * epochs
scheduler.step()  # æ¯ä¸ªä¼˜åŒ–æ­¥éª¤åæ›´æ–°
```

### 7. æ¢¯åº¦è£å‰ª

**åŠŸèƒ½**ï¼š
- âœ… ä½¿ç”¨ `torch.nn.utils.clip_grad_norm_`
- âœ… å¯é…ç½® `max_grad_norm` å‚æ•°
- âœ… é»˜è®¤å€¼ 1.0

**ä¼˜åŠ¿**ï¼š
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- æé«˜è®­ç»ƒç¨³å®šæ€§

### 8. æœ€ä½³æ¨¡å‹ä¿å­˜

**åŠŸèƒ½**ï¼š
- âœ… è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆloss æœ€ä½ï¼‰
- âœ… å®šæœŸä¿å­˜ checkpoint
- âœ… ä¿å­˜å®Œæ•´çš„è®­ç»ƒçŠ¶æ€

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”

### è®­ç»ƒé€Ÿåº¦

| é¡¹ç›® | ä¹‹å‰ | ç°åœ¨ | æå‡ |
|------|------|------|------|
| å›¾åƒå°ºå¯¸ | 256Ã—256 | 64Ã—64 | 16Ã— |
| æ··åˆç²¾åº¦ | âŒ | âœ… | 2Ã— |
| **æ€»æå‡** | - | - | **~32Ã—** |

### æ˜¾å­˜å ç”¨

| é¡¹ç›® | ä¹‹å‰ | ç°åœ¨ | å‡å°‘ |
|------|------|------|------|
| å›¾åƒå°ºå¯¸ | 256Ã—256 | 64Ã—64 | 16Ã— |
| æ··åˆç²¾åº¦ | âŒ | âœ… | 2Ã— |
| **æ€»å‡å°‘** | - | - | **~32Ã—** |

---

## ğŸ”§ å…³é”®æ”¹è¿›ç‚¹

### 1. Padding Mask æ”¯æŒ

```python
# è®­ç»ƒæ—¶åªå¯¹æœ‰æ•ˆåƒç´ è®¡ç®— loss
mask_expanded = mask.unsqueeze(1)  # [B, 1, H, W]
diff = (clean_pred_img - clean) ** 2
masked_diff = diff * mask_expanded
loss = masked_diff.sum() / (mask_expanded.sum() + 1e-8)
```

**ä¼˜åŠ¿**ï¼š
- ä¸æµªè´¹è®¡ç®—èµ„æºåœ¨ padding ä¸Š
- æ›´å‡†ç¡®çš„æ¢¯åº¦
- æ›´å¿«çš„æ”¶æ•›

### 2. æ— æ¡ä»¶é¢„è®­ç»ƒ

```python
# é¢„è®­ç»ƒæ—¶ä¸éœ€è¦ condition
clean_pred = model(noisy_target, t, condition=None, mask=mask)
```

**ä¼˜åŠ¿**ï¼š
- ç¬¦åˆ LLM é¢„è®­ç»ƒæ–¹å¼
- æ¨¡å‹å­¦ä¹ ä»å™ªå£°æ¢å¤åŸå§‹å›¾åƒ
- å¾®è°ƒæ—¶å¯ä»¥é€šè¿‡ chat_template å®ç°æ¡ä»¶ç”Ÿæˆ

### 3. å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–

```python
# è€ƒè™‘æ¢¯åº¦ç´¯ç§¯
effective_batches = len(dataloader) // gradient_accumulation_steps
total_steps = effective_batches * epochs

# æ¯ä¸ªä¼˜åŒ–æ­¥éª¤åæ›´æ–°
if (batch_idx + 1) % gradient_accumulation_steps == 0:
    optimizer.step()
    scheduler.step()  # æ­£ç¡®æ›´æ–°å­¦ä¹ ç‡
```

**ä¼˜åŠ¿**ï¼š
- å­¦ä¹ ç‡è°ƒåº¦æ›´å‡†ç¡®
- è€ƒè™‘æ¢¯åº¦ç´¯ç§¯çš„å®é™…æ­¥æ•°

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬è®­ç»ƒ

```bash
python train.py \
    --data_path ./data/train \
    --img_size 64 \
    --batch_size 32 \
    --epochs 100 \
    --use_amp \
    --gradient_accumulation_steps 4
```

### åˆ†å¸ƒå¼è®­ç»ƒ

```bash
torchrun --nproc_per_node=4 train.py \
    --data_path ./data/train \
    --img_size 64 \
    --batch_size 8 \
    --epochs 100
```

### ä»æ£€æŸ¥ç‚¹æ¢å¤

```bash
python train.py \
    --data_path ./data/train \
    --resume ./output/checkpoint_epoch_50.pth \
    --epochs 100
```

---

## âœ… æ£€æŸ¥æ¸…å•

- [x] å›¾åƒå°ºå¯¸æ”¹ä¸º 64Ã—64
- [x] ç§»é™¤é—®ç­”å¯¹æ ¼å¼ï¼Œæ”¯æŒçº¯æ–‡æœ¬
- [x] æ”¯æŒ padding mask
- [x] æ”¯æŒæ— æ¡ä»¶è®­ç»ƒ
- [x] æ··åˆç²¾åº¦è®­ç»ƒ
- [x] æ¢¯åº¦ç´¯ç§¯
- [x] æ¢¯åº¦è£å‰ª
- [x] å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–
- [x] æœ€ä½³æ¨¡å‹ä¿å­˜
- [x] è¯­æ³•æ£€æŸ¥é€šè¿‡
- [x] åŠŸèƒ½æ£€æŸ¥é€šè¿‡

---

## ğŸ¯ æ€»ç»“

âœ… **æ‰€æœ‰ä¼˜åŒ–å·²å®Œæˆ**

âœ… **è®­ç»ƒè„šæœ¬å·²é€‚é… 64Ã—64 å›¾åƒå°ºå¯¸**

âœ… **æ”¯æŒçº¯æ–‡æœ¬é¢„è®­ç»ƒå’Œ padding mask**

âœ… **æ€§èƒ½ä¼˜åŒ–ï¼ˆæ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰ï¼‰å·²å®ç°**

âœ… **ä»£ç å·²é€šè¿‡è¯­æ³•å’ŒåŠŸèƒ½æ£€æŸ¥**

---

**ä¿®æ”¹æ—¥æœŸ**: 2024-12-15
**ç‰ˆæœ¬**: v2.0
