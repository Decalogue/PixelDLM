# LLM Padding æœºåˆ¶è¯¦è§£ï¼ˆQwenã€Kimi ç­‰ï¼‰

## ğŸ¯ æ ¸å¿ƒç»“è®º

**è®­ç»ƒé˜¶æ®µ**ï¼šä½¿ç”¨ **Right Padding**ï¼ˆåœ¨åºåˆ—æœ«å°¾æ·»åŠ  paddingï¼‰  
**æ¨ç†é˜¶æ®µ**ï¼šä½¿ç”¨ **Left Padding**ï¼ˆåœ¨åºåˆ—å¼€å¤´æ·»åŠ  paddingï¼‰

---

## ğŸ“Š è®­ç»ƒé˜¶æ®µï¼šRight Padding

### ä¸ºä»€ä¹ˆè®­ç»ƒæ—¶ç”¨ Right Paddingï¼Ÿ

#### 1. ä¸å¤„ç†æ–¹å‘ä¸€è‡´

**Decoder-only æ¨¡å‹**ï¼ˆå¦‚ Qwenã€Kimiï¼‰ä»å·¦åˆ°å³å¤„ç†åºåˆ—ï¼š

```
[Token_0, Token_1, Token_2, ..., Token_N, PAD, PAD, ...]
 â†‘ æœ‰æ•ˆ token è¿ç»­ï¼Œpadding åœ¨æœ«å°¾
```

**ä¼˜åŠ¿**ï¼š
- âœ… æœ‰æ•ˆ token è¿ç»­ï¼Œç´¢å¼•ç®€å•
- âœ… ä¸ä»å·¦åˆ°å³çš„å¤„ç†æ–¹å¼ä¸€è‡´
- âœ… ä½ç½®ç¼–ç è¿ç»­ï¼ˆ0 åˆ° N-1ï¼‰

#### 2. ç®€åŒ– Loss è®¡ç®—

**è®­ç»ƒæ—¶çš„ Loss è®¡ç®—**ï¼š
```python
# Right padding
input_ids = [token_0, token_1, ..., token_N, PAD, PAD, ...]
labels = [token_1, token_2, ..., token_N+1, IGNORE, IGNORE, ...]
#                    â†‘ é¢„æµ‹ä¸‹ä¸€ä¸ª token        â†‘ padding ä½ç½®å¿½ç•¥
```

**ä¼˜åŠ¿**ï¼š
- âœ… æœ‰æ•ˆ token çš„ labels è¿ç»­
- âœ… Padding ä½ç½®ç”¨ `IGNORE_INDEX` æ ‡è®°ï¼Œä¸å‚ä¸ loss è®¡ç®—
- âœ… å®ç°ç®€å•

#### 3. Attention Mask

**Right Padding çš„ Attention Mask**ï¼š
```python
attention_mask = [1, 1, ..., 1, 0, 0, ..., 0]
                 â†‘ æœ‰æ•ˆ token    â†‘ padding
```

**ç‰¹ç‚¹**ï¼š
- âœ… æœ‰æ•ˆ token å¯ä»¥äº’ç›¸ attention
- âœ… Padding ä½ç½®è¢« mask æ‰ï¼Œä¸å‚ä¸ attention
- âœ… è®¡ç®—æ•ˆç‡é«˜

---

## ğŸ”„ æ¨ç†é˜¶æ®µï¼šLeft Padding

### ä¸ºä»€ä¹ˆæ¨ç†æ—¶ç”¨ Left Paddingï¼Ÿ

#### 1. å› æœæ³¨æ„åŠ›æœºåˆ¶

**Decoder-only æ¨¡å‹ä½¿ç”¨å› æœæ³¨æ„åŠ›**ï¼š
- æ¯ä¸ª token åªèƒ½çœ‹åˆ°**å‰é¢çš„ token**
- é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œéœ€è¦**æœ€åä¸€ä¸ª token æ˜¯æœ‰æ•ˆ token**

**Right Padding çš„é—®é¢˜**ï¼š
```
[Token_0, Token_1, ..., Token_N, PAD, PAD, ...]
                                    â†‘ æœ€åä¸€ä¸ª token æ˜¯ PAD
                                    â†‘ æ¨¡å‹ä¼šåŸºäº PAD é¢„æµ‹ï¼Œå¯¼è‡´é”™è¯¯
```

**Left Padding çš„è§£å†³æ–¹æ¡ˆ**ï¼š
```
[PAD, PAD, ..., Token_0, Token_1, ..., Token_N]
 â†‘ padding    â†‘ æœ€åä¸€ä¸ª token æ˜¯æœ‰æ•ˆ token
              â†‘ æ¨¡å‹åŸºäºæœ‰æ•ˆ token é¢„æµ‹ï¼Œæ­£ç¡®
```

#### 2. æ‰¹é‡ç”Ÿæˆ

**æ‰¹é‡ç”Ÿæˆæ—¶ï¼Œä¸åŒåºåˆ—é•¿åº¦ä¸åŒ**ï¼š

**Right Padding çš„é—®é¢˜**ï¼š
```python
# Batch ä¸­çš„åºåˆ—
seq_1: [token_0, token_1, token_2, PAD, PAD]  # é•¿åº¦ 3
seq_2: [token_0, token_1, PAD, PAD, PAD]     # é•¿åº¦ 2

# ç”Ÿæˆæ—¶ï¼Œæœ€åä¸€ä¸ª token
seq_1 çš„æœ€åä¸€ä¸ª token: PAD  âŒ
seq_2 çš„æœ€åä¸€ä¸ª token: PAD  âŒ
# ä¸¤ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ª token éƒ½æ˜¯ PADï¼Œæ— æ³•æ­£ç¡®ç”Ÿæˆ
```

**Left Padding çš„è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# Batch ä¸­çš„åºåˆ—
seq_1: [PAD, PAD, token_0, token_1, token_2]  # é•¿åº¦ 3
seq_2: [PAD, PAD, PAD, token_0, token_1]      # é•¿åº¦ 2

# ç”Ÿæˆæ—¶ï¼Œæœ€åä¸€ä¸ª token
seq_1 çš„æœ€åä¸€ä¸ª token: token_2  âœ…
seq_2 çš„æœ€åä¸€ä¸ª token: token_1  âœ…
# æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ª token éƒ½æ˜¯æœ‰æ•ˆ tokenï¼Œå¯ä»¥æ­£ç¡®ç”Ÿæˆ
```

#### 3. ä½ç½®ç¼–ç 

**Left Padding çš„ä½ç½®ç¼–ç **ï¼š
```python
# åºåˆ—ï¼š[PAD, PAD, token_0, token_1, token_2]
# ä½ç½®ç¼–ç ï¼š[0, 1, 2, 3, 4]  # ä½† PAD ä½ç½®è¢« mask æ‰
# å®é™…æœ‰æ•ˆä½ç½®ï¼š[2, 3, 4]  # token_0, token_1, token_2
```

**æ³¨æ„**ï¼š
- âš ï¸ ä½ç½®ç¼–ç ä» 0 å¼€å§‹ï¼Œä½†æœ‰æ•ˆ token çš„ä½ç½®ä¸è¿ç»­
- âš ï¸ éœ€è¦ä½¿ç”¨ `position_ids` æ¥æ­£ç¡®è®¾ç½®ä½ç½®ç¼–ç 
- âš ï¸ æˆ–è€…ä½¿ç”¨ RoPEï¼ˆRotary Position Embeddingï¼‰ï¼Œå¯ä»¥å¤„ç†ç›¸å¯¹ä½ç½®

---

## ğŸ”§ Qwen çš„å…·ä½“å®ç°

### è®­ç»ƒæ—¶é…ç½®

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# è®­ç»ƒæ—¶ï¼šright padding
tokenizer.padding_side = "right"  # é»˜è®¤å€¼

# Tokenize
inputs = tokenizer(
    ["æ–‡æœ¬1", "æ–‡æœ¬2"],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt"
)

# ç»“æœ
# input_ids: [[token_0, ..., token_N, PAD, PAD], [token_0, ..., token_M, PAD, PAD]]
# attention_mask: [[1, ..., 1, 0, 0], [1, ..., 1, 0, 0]]
```

### æ¨ç†æ—¶é…ç½®

```python
# æ¨ç†æ—¶ï¼šleft padding
tokenizer.padding_side = "left"

# Tokenize
inputs = tokenizer(
    ["æ–‡æœ¬1", "æ–‡æœ¬2"],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt"
)

# ç»“æœ
# input_ids: [[PAD, PAD, token_0, ..., token_N], [PAD, PAD, PAD, token_0, ..., token_M]]
# attention_mask: [[0, 0, 1, ..., 1], [0, 0, 0, 1, ..., 1]]
```

### åŠ¨æ€åˆ‡æ¢

```python
# è®­ç»ƒæ—¶
tokenizer.padding_side = "right"
# ... è®­ç»ƒä»£ç  ...

# æ¨ç†æ—¶
tokenizer.padding_side = "left"
# ... æ¨ç†ä»£ç  ...
```

---

## ğŸ“Š å¯¹æ¯”æ€»ç»“

### Right Paddingï¼ˆè®­ç»ƒï¼‰

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ä½ç½®** | åºåˆ—æœ«å°¾ |
| **æœ‰æ•ˆ token** | è¿ç»­ï¼ˆä»ä½ç½® 0 å¼€å§‹ï¼‰ |
| **ä½ç½®ç¼–ç ** | è¿ç»­ï¼ˆ0 åˆ° N-1ï¼‰ |
| **Loss è®¡ç®—** | ç®€å•ï¼ˆæœ‰æ•ˆ token è¿ç»­ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | è®­ç»ƒé˜¶æ®µ |

**åºåˆ—ç»“æ„**ï¼š
```
[Token_0, Token_1, ..., Token_N, PAD, PAD, ...]
 â†‘ æœ‰æ•ˆ token è¿ç»­                â†‘ padding
```

---

### Left Paddingï¼ˆæ¨ç†ï¼‰

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ä½ç½®** | åºåˆ—å¼€å¤´ |
| **æœ‰æ•ˆ token** | ä¸è¿ç»­ï¼ˆä»ä½ç½® M å¼€å§‹ï¼‰ |
| **ä½ç½®ç¼–ç ** | éœ€è¦è°ƒæ•´ï¼ˆæˆ–ä½¿ç”¨ RoPEï¼‰ |
| **æœ€åä¸€ä¸ª token** | æ€»æ˜¯æœ‰æ•ˆ token âœ… |
| **é€‚ç”¨åœºæ™¯** | æ¨ç†é˜¶æ®µï¼ˆæ‰¹é‡ç”Ÿæˆï¼‰ |

**åºåˆ—ç»“æ„**ï¼š
```
[PAD, PAD, ..., Token_0, Token_1, ..., Token_N]
 â†‘ padding    â†‘ æœ‰æ•ˆ tokenï¼ˆæœ€åä¸€ä¸ªæ€»æ˜¯æœ‰æ•ˆï¼‰
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### 1. æ¨ç†æ—¶ä½¿ç”¨ Right Padding ä¼šæ€æ ·ï¼Ÿ

**é—®é¢˜**ï¼š
- æœ€åä¸€ä¸ª token å¯èƒ½æ˜¯ PAD
- æ¨¡å‹åŸºäº PAD é¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼Œå¯¼è‡´é”™è¯¯è¾“å‡º
- æ‰¹é‡ç”Ÿæˆæ—¶ï¼Œä¸åŒåºåˆ—çš„æœ€åä¸€ä¸ª token éƒ½æ˜¯ PAD

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ¨ç†æ—¶åˆ‡æ¢åˆ° left padding
- æˆ–è€…ä½¿ç”¨ `generate()` æ–¹æ³•ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†

### 2. ä½ç½®ç¼–ç å¦‚ä½•å¤„ç†ï¼Ÿ

**Right Padding**ï¼š
- ä½ç½®ç¼–ç è¿ç»­ï¼ˆ0 åˆ° N-1ï¼‰
- å®ç°ç®€å•

**Left Padding**ï¼š
- ä½ç½®ç¼–ç ä¸è¿ç»­ï¼ˆæœ‰æ•ˆ token ä»ä½ç½® M å¼€å§‹ï¼‰
- éœ€è¦ï¼š
  - ä½¿ç”¨ `position_ids` æ‰‹åŠ¨è®¾ç½®
  - æˆ–ä½¿ç”¨ RoPEï¼ˆå¯ä»¥å¤„ç†ç›¸å¯¹ä½ç½®ï¼‰

### 3. Attention Mask çš„åŒºåˆ«ï¼Ÿ

**Right Padding**ï¼š
```python
attention_mask = [1, 1, ..., 1, 0, 0, ..., 0]
                 â†‘ æœ‰æ•ˆ token    â†‘ padding
```

**Left Padding**ï¼š
```python
attention_mask = [0, 0, ..., 0, 1, 1, ..., 1]
                 â†‘ padding    â†‘ æœ‰æ•ˆ token
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### è®­ç»ƒé˜¶æ®µ

```python
# 1. è®¾ç½® right padding
tokenizer.padding_side = "right"

# 2. Tokenize
inputs = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

# 3. åˆ›å»º labelsï¼ˆshift by 1ï¼‰
labels = inputs["input_ids"].clone()
labels[inputs["attention_mask"] == 0] = IGNORE_INDEX  # å¿½ç•¥ padding

# 4. è®­ç»ƒ
loss = model(input_ids=inputs["input_ids"], labels=labels).loss
```

### æ¨ç†é˜¶æ®µ

```python
# 1. è®¾ç½® left padding
tokenizer.padding_side = "left"

# 2. Tokenize
inputs = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

# 3. ç”Ÿæˆ
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
)
```

---

## ğŸ“ ä¸ JiT æ¨¡å‹çš„å¯¹æ¯”

### JiT æ¨¡å‹çš„æ¡ä»¶ Padding

**å½“å‰å®ç°**ï¼šRight Paddingï¼ˆæ¡ä»¶åé¢æ·»åŠ  paddingï¼‰

```python
# æ¡ä»¶åœ¨åºåˆ—å‰é¢ï¼Œpadding åœ¨åé¢
cond_embed = torch.cat([cond_embed, cond_padding], dim=1)  # right padding
```

**åºåˆ—ç»“æ„**ï¼š
```
[Cond_real, PAD, PAD, ..., Target_0, Target_1, ..., Target_255]
 â†‘ å®é™…æ¡ä»¶    â†‘ padding      â†‘ ç›®æ ‡å›¾åƒ
```

### ä¸ LLM çš„å¯¹æ¯”

| ç‰¹æ€§ | LLMï¼ˆè®­ç»ƒï¼‰ | LLMï¼ˆæ¨ç†ï¼‰ | JiTï¼ˆæ¡ä»¶ï¼‰ |
|------|------------|------------|------------|
| **Padding ä½ç½®** | Right | Left | Right |
| **åŸå› ** | æœ‰æ•ˆ token è¿ç»­ | æœ€åä¸€ä¸ª token æœ‰æ•ˆ | æ¡ä»¶åœ¨åºåˆ—å‰é¢ |
| **ä½ç½®ç¼–ç ** | è¿ç»­ | éœ€è¦è°ƒæ•´ | è¿ç»­ |
| **é€‚ç”¨åœºæ™¯** | è®­ç»ƒ | æ¨ç† | æ¡ä»¶ç”Ÿæˆ |

**å…³é”®åŒºåˆ«**ï¼š
- **LLM**ï¼šè®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ä¸åŒçš„ padding ç­–ç•¥
- **JiT**ï¼šæ¡ä»¶ç”Ÿæˆæ—¶ï¼Œæ¡ä»¶åœ¨åºåˆ—å‰é¢ï¼Œæ‰€ä»¥ç”¨ right paddingï¼ˆä¸ LLM è®­ç»ƒæ—¶ç±»ä¼¼ï¼‰

---

## âœ… æ€»ç»“

### LLM Padding æœºåˆ¶

1. **è®­ç»ƒé˜¶æ®µ**ï¼šRight Padding
   - æœ‰æ•ˆ token è¿ç»­
   - ä½ç½®ç¼–ç è¿ç»­
   - Loss è®¡ç®—ç®€å•

2. **æ¨ç†é˜¶æ®µ**ï¼šLeft Padding
   - æœ€åä¸€ä¸ª token æ€»æ˜¯æœ‰æ•ˆ
   - æ”¯æŒæ‰¹é‡ç”Ÿæˆ
   - éœ€è¦å¤„ç†ä½ç½®ç¼–ç 

### JiT æ¨¡å‹

- **æ¡ä»¶ç”Ÿæˆ**ï¼šRight Paddingï¼ˆæ¡ä»¶åœ¨åºåˆ—å‰é¢ï¼‰
- **ä¸ LLM è®­ç»ƒæ—¶ç±»ä¼¼**ï¼šæœ‰æ•ˆ token è¿ç»­ï¼Œä½ç½®ç¼–ç è¿ç»­

---

**æ›´æ–°æ—¥æœŸ**: 2025-12-15
