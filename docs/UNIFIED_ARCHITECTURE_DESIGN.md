# ç»Ÿä¸€æ¶æ„è®¾è®¡ï¼šå›ºå®šåºåˆ—é•¿åº¦

## ğŸ¯ é—®é¢˜

**å½“å‰é—®é¢˜**ï¼š
- æ— æ¡ä»¶ç”Ÿæˆï¼šåºåˆ—é•¿åº¦ = 256
- æ¡ä»¶ç”Ÿæˆï¼šåºåˆ—é•¿åº¦ = 512ï¼ˆ256 + 256ï¼‰
- **æ¶æ„ä¸ä¸€è‡´**ï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒä½¿ç”¨ä¸åŒçš„åºåˆ—é•¿åº¦

---

## âœ… è§£å†³æ–¹æ¡ˆï¼šå›ºå®šæœ€å¤§åºåˆ—é•¿åº¦

### è®¾è®¡æ€è·¯

**ç»Ÿä¸€æ¶æ„**ï¼šæ— è®ºæ˜¯å¦æœ‰æ¡ä»¶ï¼Œéƒ½ä½¿ç”¨**å›ºå®šçš„æœ€å¤§åºåˆ—é•¿åº¦**ï¼ˆ512ï¼‰

**å®ç°æ–¹å¼**ï¼š
1. **æ— æ¡ä»¶æ—¶**ï¼šå‰ 256 ä¸ªä½ç½®ä½¿ç”¨ paddingï¼ˆæˆ–ç‰¹æ®Š tokenï¼‰ï¼Œå 256 ä¸ªæ˜¯ç›®æ ‡å›¾åƒ
2. **æœ‰æ¡ä»¶æ—¶**ï¼šå‰ 256 ä¸ªæ˜¯æ¡ä»¶ï¼Œå 256 ä¸ªæ˜¯ç›®æ ‡å›¾åƒ
3. **ä½¿ç”¨ attention mask**ï¼šæ§åˆ¶å“ªäº›ä½ç½®å‚ä¸è®¡ç®—

---

## ğŸ“Š æ¶æ„è®¾è®¡

### å›ºå®šåºåˆ—ç»“æ„

```
æ€»åºåˆ—é•¿åº¦ï¼š512 patchesï¼ˆå›ºå®šï¼‰

æ— æ¡ä»¶ç”Ÿæˆï¼š
[PAD, PAD, ..., PAD, | Target_0, Target_1, ..., Target_255]
  â†‘ 256 ä¸ª padding    â†‘ 256 ä¸ªç›®æ ‡ patches

æ¡ä»¶ç”Ÿæˆï¼š
[Cond_0, Cond_1, ..., Cond_255, | Target_0, Target_1, ..., Target_255]
  â†‘ 256 ä¸ªæ¡ä»¶ patches          â†‘ 256 ä¸ªç›®æ ‡ patches
```

### Attention Mask

```python
# æ— æ¡ä»¶ï¼šmask æ‰å‰ 256 ä¸ªä½ç½®
mask_uncond = [0, 0, ..., 0, | 1, 1, ..., 1]
               â†‘ 256 ä¸ª 0    â†‘ 256 ä¸ª 1

# æ¡ä»¶ï¼šå‰ 256 ä¸ªä½ç½®æœ‰æ•ˆ
mask_cond = [1, 1, ..., 1, | 1, 1, ..., 1]
            â†‘ 256 ä¸ª 1     â†‘ 256 ä¸ª 1
```

---

## ğŸ”§ å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šPadding + Attention Maskï¼ˆæ¨èï¼‰â­

**è®¾è®¡**ï¼š
- æ— æ¡ä»¶æ—¶ï¼šå‰ 256 ä¸ªä½ç½®ä½¿ç”¨é›¶ padding
- ä½¿ç”¨ attention mask æ§åˆ¶è®¡ç®—
- æ¶æ„å®Œå…¨ç»Ÿä¸€

**ä¼˜ç‚¹**ï¼š
- âœ… æ¶æ„å®Œå…¨ä¸€è‡´
- âœ… é¢„è®­ç»ƒå’Œå¾®è°ƒä½¿ç”¨ç›¸åŒæ¶æ„
- âœ… å®ç°ç®€å•

**ç¼ºç‚¹**ï¼š
- âš ï¸ æ— æ¡ä»¶æ—¶å‰ 256 ä¸ªä½ç½®ä»ä¼šè®¡ç®—ï¼ˆä½†ä¼šè¢« mask æ‰ï¼Œå®é™…ä¸å‚ä¸æ³¨æ„åŠ›ï¼‰

---

### æ–¹æ¡ˆ 2ï¼šç‰¹æ®Š Tokenï¼ˆæ›´ä¼˜é›…ï¼‰

**è®¾è®¡**ï¼š
- æ— æ¡ä»¶æ—¶ï¼šå‰ 256 ä¸ªä½ç½®ä½¿ç”¨ç‰¹æ®Šçš„"æ— æ¡ä»¶" token
- æ¨¡å‹å­¦ä¹ åŒºåˆ†"æ— æ¡ä»¶"å’Œ"çœŸå®æ¡ä»¶"

**ä¼˜ç‚¹**ï¼š
- âœ… æ¶æ„å®Œå…¨ä¸€è‡´
- âœ… æ¨¡å‹å¯ä»¥å­¦ä¹ "æ— æ¡ä»¶"çš„ç‰¹æ®Šè¡¨ç¤º

**ç¼ºç‚¹**ï¼š
- âš ï¸ éœ€è¦é¢å¤–çš„å­¦ä¹ è¿‡ç¨‹

---

## ğŸ¯ æ¨èå®ç°ï¼šæ–¹æ¡ˆ 1

### æ¶æ„ä¿®æ”¹

```python
class JiT(nn.Module):
    def __init__(self, ...):
        # å›ºå®šæœ€å¤§åºåˆ—é•¿åº¦
        self.max_seq_len = 512  # 256 (æ¡ä»¶) + 256 (ç›®æ ‡)
        
        # ç›®æ ‡å›¾åƒ patches
        self.num_patches = 256
        
        # 2D ä½ç½®ç¼–ç ï¼ˆæ‰©å±•åˆ°æœ€å¤§åºåˆ—é•¿åº¦ï¼‰
        # å‰ 256 ä¸ªï¼šæ¡ä»¶ä½ç½®ï¼ˆ16Ã—16ï¼‰
        # å 256 ä¸ªï¼šç›®æ ‡ä½ç½®ï¼ˆ16Ã—16ï¼‰
        max_grid_size = 16  # 16Ã—16 = 256
        self.pos_embed_h = nn.Parameter(torch.zeros(1, max_grid_size, embed_dim))
        self.pos_embed_w = nn.Parameter(torch.zeros(1, max_grid_size, embed_dim))
        
        # æ¡ä»¶ä½ç½®ç¼–ç ï¼ˆå‰ 256 ä¸ªä½ç½®ï¼‰
        self.cond_pos_embed_h = nn.Parameter(torch.zeros(1, max_grid_size, embed_dim))
        self.cond_pos_embed_w = nn.Parameter(torch.zeros(1, max_grid_size, embed_dim))
    
    def forward(self, x, t, condition=None, mask=None):
        B = x.shape[0]
        
        # ç›®æ ‡å›¾åƒåµŒå…¥å’Œä½ç½®ç¼–ç 
        x_target = self.patch_embed(x_patches)  # [B, 256, embed_dim]
        x_target = x_target + self._get_2d_pos_embed(256, self.pos_embed_h, self.pos_embed_w)
        
        # æ¡ä»¶å¤„ç†
        if condition is not None:
            # æœ‰æ¡ä»¶ï¼šä½¿ç”¨çœŸå®æ¡ä»¶
            cond_embed = self.condition_embed(condition)  # [B, cond_patches, embed_dim]
            cond_embed = cond_embed + self._get_2d_pos_embed(cond_patches, self.cond_pos_embed_h, self.cond_pos_embed_w)
            # å¦‚æœæ¡ä»¶ patches < 256ï¼Œéœ€è¦ padding
            if cond_embed.shape[1] < 256:
                padding = 256 - cond_embed.shape[1]
                cond_padding = torch.zeros(B, padding, self.embed_dim, device=cond_embed.device)
                cond_embed = torch.cat([cond_embed, cond_padding], dim=1)
        else:
            # æ— æ¡ä»¶ï¼šä½¿ç”¨é›¶ padding
            cond_embed = torch.zeros(B, 256, self.embed_dim, device=x_target.device)
        
        # æ‹¼æ¥ï¼šæ€»é•¿åº¦å›ºå®šä¸º 512
        x = torch.cat([cond_embed, x_target], dim=1)  # [B, 512, embed_dim]
        
        # åˆ›å»º attention mask
        if condition is not None:
            # æœ‰æ¡ä»¶ï¼šå‰ 256 ä¸ªä½ç½®æœ‰æ•ˆï¼ˆæ ¹æ®å®é™…æ¡ä»¶é•¿åº¦ï¼‰
            cond_mask = torch.ones(B, condition.shape[1], device=x.device)
            if condition.shape[1] < 256:
                cond_padding_mask = torch.zeros(B, 256 - condition.shape[1], device=x.device)
                cond_mask = torch.cat([cond_mask, cond_padding_mask], dim=1)
            target_mask = torch.ones(B, 256, device=x.device)
            attention_mask = torch.cat([cond_mask, target_mask], dim=1)  # [B, 512]
        else:
            # æ— æ¡ä»¶ï¼šå‰ 256 ä¸ªä½ç½®æ— æ•ˆï¼ˆpaddingï¼‰
            cond_mask = torch.zeros(B, 256, device=x.device)
            target_mask = torch.ones(B, 256, device=x.device)
            attention_mask = torch.cat([cond_mask, target_mask], dim=1)  # [B, 512]
        
        # Transformer blocksï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ attention maskï¼‰
        for block in self.blocks:
            x = block(x, t_embed, attention_mask=attention_mask)
        
        # æå–ç›®æ ‡éƒ¨åˆ†ï¼ˆå 256 ä¸ªä½ç½®ï¼‰
        x = x[:, 256:, :]  # [B, 256, embed_dim]
        
        return x
```

---

## ğŸ“Š æ¶æ„å¯¹æ¯”

### ä¿®æ”¹å‰

```
æ— æ¡ä»¶ï¼š
åºåˆ—é•¿åº¦: 256
æ¶æ„: [Target_0, ..., Target_255]

æ¡ä»¶ï¼š
åºåˆ—é•¿åº¦: 320-512ï¼ˆå¯å˜ï¼‰
æ¶æ„: [Cond_0, ..., Cond_N, Target_0, ..., Target_255]
```

### ä¿®æ”¹å

```
æ— æ¡ä»¶ï¼š
åºåˆ—é•¿åº¦: 512ï¼ˆå›ºå®šï¼‰
æ¶æ„: [PAD_0, ..., PAD_255, Target_0, ..., Target_255]
Mask:  [0, ..., 0, 1, ..., 1]

æ¡ä»¶ï¼š
åºåˆ—é•¿åº¦: 512ï¼ˆå›ºå®šï¼‰
æ¶æ„: [Cond_0, ..., Cond_255, Target_0, ..., Target_255]
Mask:  [1, ..., 1, 1, ..., 1]
```

---

## âœ… ä¼˜åŠ¿

### 1. æ¶æ„å®Œå…¨ç»Ÿä¸€

- âœ… é¢„è®­ç»ƒå’Œå¾®è°ƒä½¿ç”¨ç›¸åŒçš„åºåˆ—é•¿åº¦ï¼ˆ512ï¼‰
- âœ… ä½ç½®ç¼–ç ç»Ÿä¸€
- âœ… Transformer blocks å¤„ç†ç›¸åŒçš„è¾“å…¥ç»´åº¦

### 2. è®­ç»ƒä¸€è‡´æ€§

- âœ… æ¨¡å‹ä»ä¸€å¼€å§‹å°±é€‚åº” 512 é•¿åº¦çš„åºåˆ—
- âœ… ä½ç½®ç¼–ç å­¦ä¹ æ¡ä»¶+ç›®æ ‡çš„ç»„åˆæ¨¡å¼
- âœ… æ— éœ€åœ¨å¾®è°ƒæ—¶é€‚åº”æ–°çš„åºåˆ—é•¿åº¦

### 3. å®ç°ç®€å•

- âœ… åªéœ€ä¿®æ”¹ forward æ–¹æ³•
- âœ… ä½¿ç”¨ attention mask æ§åˆ¶è®¡ç®—
- âœ… æ— éœ€ä¿®æ”¹ Transformer blocks

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. è®¡ç®—å¼€é”€

- æ— æ¡ä»¶æ—¶å‰ 256 ä¸ªä½ç½®ä»ä¼šè®¡ç®—ï¼ˆä½†ä¼šè¢« mask æ‰ï¼‰
- å¯ä»¥é€šè¿‡ä¼˜åŒ– attention å®ç°å‡å°‘è®¡ç®—ï¼ˆå¦‚ Flash Attentionï¼‰

### 2. ä½ç½®ç¼–ç 

- å‰ 256 ä¸ªä½ç½®ï¼šæ¡ä»¶ä½ç½®ç¼–ç 
- å 256 ä¸ªä½ç½®ï¼šç›®æ ‡ä½ç½®ç¼–ç 
- éœ€è¦ç¡®ä¿ä½ç½®ç¼–ç æ­£ç¡®

### 3. é¢„è®­ç»ƒç­–ç•¥

- é¢„è®­ç»ƒæ—¶ä¹Ÿä½¿ç”¨å›ºå®š 512 é•¿åº¦
- æ— æ¡ä»¶æ—¶å‰ 256 ä¸ªæ˜¯ padding
- è®©æ¨¡å‹ä»ä¸€å¼€å§‹å°±é€‚åº”è¿™ç§æ¶æ„

---

## ğŸ“ å®ç°æ­¥éª¤

### Step 1: ä¿®æ”¹æ¨¡å‹åˆå§‹åŒ–

```python
def __init__(self, ...):
    # å›ºå®šæœ€å¤§åºåˆ—é•¿åº¦
    self.max_seq_len = 512
    self.num_patches = 256
    self.cond_max_patches = 256
```

### Step 2: ä¿®æ”¹ forward æ–¹æ³•

- æ— æ¡ä»¶æ—¶ï¼šå‰ 256 ä¸ªä½ç½®ä½¿ç”¨é›¶ padding
- æœ‰æ¡ä»¶æ—¶ï¼šå‰ 256 ä¸ªä½ç½®ä½¿ç”¨çœŸå®æ¡ä»¶ï¼ˆå¯èƒ½ paddingï¼‰
- æ€»é•¿åº¦å›ºå®šä¸º 512

### Step 3: ç»Ÿä¸€ attention mask

- æ— æ¡ä»¶ï¼šå‰ 256 ä¸ªä½ç½® mask æ‰
- æœ‰æ¡ä»¶ï¼šæ ¹æ®å®é™…æ¡ä»¶é•¿åº¦è®¾ç½® mask

---

## âœ… æ€»ç»“

### é—®é¢˜å›ç­”

**èƒ½å¦ç»Ÿä¸€æ¶æ„ï¼Ÿ**

**å¯ä»¥ï¼** é€šè¿‡å›ºå®šæœ€å¤§åºåˆ—é•¿åº¦ä¸º 512ï¼Œä½¿ç”¨ padding + attention maskã€‚

### æ¨èæ–¹æ¡ˆ

**æ–¹æ¡ˆ 1ï¼šPadding + Attention Mask**
- æ¶æ„å®Œå…¨ç»Ÿä¸€
- å®ç°ç®€å•
- é¢„è®­ç»ƒå’Œå¾®è°ƒä½¿ç”¨ç›¸åŒæ¶æ„

### å…³é”®ç‚¹

1. âœ… å›ºå®šåºåˆ—é•¿åº¦ï¼š512ï¼ˆ256 æ¡ä»¶ + 256 ç›®æ ‡ï¼‰
2. âœ… æ— æ¡ä»¶æ—¶ï¼šå‰ 256 ä¸ªä½ç½®æ˜¯ padding
3. âœ… ä½¿ç”¨ attention mask æ§åˆ¶è®¡ç®—
4. âœ… é¢„è®­ç»ƒæ—¶ä¹Ÿä½¿ç”¨è¿™ç§æ¶æ„

---

**ä¿®æ”¹æ—¥æœŸ**: 2025-12-15




