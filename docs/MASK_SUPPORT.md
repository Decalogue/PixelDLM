# Padding Mask æ”¯æŒè¯´æ˜

## ğŸ¯ æ¦‚è¿°

`model_jit.py` ç°åœ¨å®Œå…¨æ”¯æŒ padding maskï¼Œå¯ä»¥åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶æ­£ç¡®å¤„ç† padding åŒºåŸŸã€‚

---

## ğŸ”§ å®ç°ç»†èŠ‚

### 1. å›¾åƒ Mask åˆ° Patch Mask è½¬æ¢

```python
def image_mask_to_patch_mask(self, mask: torch.Tensor) -> torch.Tensor:
    """
    å°†å›¾åƒ mask è½¬æ¢ä¸º patch mask
    
    Args:
        mask: Image mask [B, H, W] - 1 è¡¨ç¤ºæœ‰æ•ˆåƒç´ ï¼Œ0 è¡¨ç¤º padding
    
    Returns:
        patch_mask: Patch mask [B, num_patches] - 1 è¡¨ç¤ºæœ‰æ•ˆ patchï¼Œ0 è¡¨ç¤º padding patch
    """
    # å°† mask é‡å¡‘ä¸º patchesï¼Œå¯¹æ¯ä¸ª patch å–å¹³å‡å€¼
    # å¦‚æœ patch ä¸­å¤§éƒ¨åˆ†åƒç´ æ˜¯æœ‰æ•ˆçš„ï¼Œåˆ™è®¤ä¸º patch æ˜¯æœ‰æ•ˆçš„
    mask_patches = mask.reshape(B, n, p, n, p)
    mask_patches = mask_patches.mean(dim=(2, 4))
    patch_mask = (mask_patches > 0.5).float()
    
    return patch_mask
```

**åŸç†**ï¼š
- å°†å›¾åƒ mask `[B, H, W]` è½¬æ¢ä¸º patch mask `[B, num_patches]`
- å¯¹æ¯ä¸ª patch å†…çš„åƒç´ å–å¹³å‡å€¼
- å¦‚æœå¹³å‡å€¼ > 0.5ï¼Œåˆ™è®¤ä¸º patch æ˜¯æœ‰æ•ˆçš„

### 2. Attention Mask æ”¯æŒ

```python
class MultiHeadAttention(nn.Module):
    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        # ...
        attn = (q @ k.transpose(-2, -1)) * self.scale
        
        # Apply attention mask if provided
        if attention_mask is not None:
            mask_expanded = attention_mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, N]
            # Mask out padding positions: set to large negative value before softmax
            attn = attn.masked_fill(mask_expanded == 0, float('-inf'))
        
        attn = attn.softmax(dim=-1)
        # ...
```

**åŸç†**ï¼š
- åœ¨ attention è®¡ç®—ä¸­ï¼Œå°† padding patches çš„ attention æƒé‡è®¾ä¸º `-inf`
- ç»è¿‡ softmax åï¼Œpadding patches çš„ attention æƒé‡ä¸º 0
- æ¨¡å‹ä¸ä¼šå…³æ³¨ padding åŒºåŸŸ

### 3. Forward æ–¹æ³•æ”¯æŒ Mask

```python
def forward(
    self,
    x: torch.Tensor,
    t: torch.Tensor,
    condition: Optional[torch.Tensor] = None,
    mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    # Convert image mask to patch mask
    if mask is not None:
        patch_mask = self.image_mask_to_patch_mask(mask)
        # å¦‚æœæœ‰ conditionï¼Œéœ€è¦æ‰©å±• patch_mask
        if condition is not None:
            cond_mask = torch.ones(B, condition.shape[1], device=patch_mask.device)
            patch_mask = torch.cat([cond_mask, patch_mask], dim=1)
    
    # Transformer blocks (with attention mask)
    for block in self.blocks:
        x = block(x, t_embed, attention_mask=patch_mask)
    
    # å°† padding patches çš„è¾“å‡ºç½®é›¶
    if patch_mask is not None:
        patch_mask_expanded = patch_mask.unsqueeze(-1)
        x = x * patch_mask_expanded
    
    return x
```

**å…³é”®ç‚¹**ï¼š
1. å°†å›¾åƒ mask è½¬æ¢ä¸º patch mask
2. åœ¨ attention ä¸­ä½¿ç”¨ patch mask
3. åœ¨è¾“å‡ºæ—¶å°† padding patches ç½®é›¶

---

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### è®­ç»ƒæ—¶

```python
# åœ¨ train.py ä¸­
for batch in dataloader:
    clean = batch['clean'].to(device)  # [B, 3, H, W]
    mask = batch['mask'].to(device)    # [B, H, W]
    
    # æ·»åŠ å™ªå£°
    noisy_target, _ = add_noise_to_timestep(clean, t)
    
    # Forward pass with mask
    clean_pred = model(noisy_target, t, condition=None, mask=mask)
    
    # Convert to image
    clean_pred_img = model.patches_to_image(clean_pred)
    
    # Loss with mask (åªå¯¹æœ‰æ•ˆåƒç´ è®¡ç®—)
    mask_expanded = mask.unsqueeze(1)  # [B, 1, H, W]
    diff = (clean_pred_img - clean) ** 2
    masked_diff = diff * mask_expanded
    loss = masked_diff.sum() / (mask_expanded.sum() + 1e-8)
```

### æ¨ç†æ—¶ï¼ˆå¯é€‰ï¼‰

```python
# åœ¨ç”Ÿæˆæ—¶ä¹Ÿå¯ä»¥ä½¿ç”¨ mask
# ä¾‹å¦‚ï¼šåªç”Ÿæˆå‰ N ä¸ª tokensï¼Œåé¢çš„ä¿æŒä¸º padding

mask = torch.ones(B, H, W)
mask[:, num_tokens//H:, :] = 0  # åé¢çš„åƒç´ æ˜¯ padding

# ç”Ÿæˆæ—¶ä¼ å…¥ mask
generated = model.generate(condition=condition, mask=mask)
```

---

## âœ… ä¼˜åŠ¿

### 1. è®­ç»ƒæ•ˆç‡

- âœ… **ä¸æµªè´¹è®¡ç®—èµ„æº**ï¼šæ¨¡å‹ä¸ä¼šå…³æ³¨ padding åŒºåŸŸ
- âœ… **æ›´å‡†ç¡®çš„æ¢¯åº¦**ï¼šåªå¯¹æœ‰æ•ˆåƒç´ è®¡ç®— loss
- âœ… **æ›´å¿«çš„æ”¶æ•›**ï¼šæ¨¡å‹ä¸“æ³¨äºå­¦ä¹ æœ‰æ•ˆå†…å®¹

### 2. æ¨¡å‹æ€§èƒ½

- âœ… **æ›´å¥½çš„è¡¨ç¤º**ï¼šæ¨¡å‹å­¦ä¹ åŒºåˆ†æœ‰æ•ˆå†…å®¹å’Œ padding
- âœ… **æ›´å‡†ç¡®çš„ç”Ÿæˆ**ï¼šç”Ÿæˆæ—¶çŸ¥é“å“ªäº›åŒºåŸŸæ˜¯ padding
- âœ… **æ›´ç¨³å®šçš„è®­ç»ƒ**ï¼šé¿å… padding åŒºåŸŸçš„å™ªå£°å½±å“

### 3. çµæ´»æ€§

- âœ… **æ”¯æŒå˜é•¿åºåˆ—**ï¼šä¸åŒæ ·æœ¬å¯ä»¥æœ‰ä¸åŒçš„æœ‰æ•ˆé•¿åº¦
- âœ… **æ”¯æŒæ¡ä»¶ç”Ÿæˆ**ï¼šå¯ä»¥ä¸ condition ä¸€èµ·ä½¿ç”¨
- âœ… **æ”¯æŒæ‰¹é‡è®­ç»ƒ**ï¼šbatch ä¸­çš„æ ·æœ¬å¯ä»¥æœ‰ä¸åŒçš„é•¿åº¦

---

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### 1. Patch Mask è½¬æ¢

```
å›¾åƒ mask: [B, 256, 256]
    â†“
é‡å¡‘ä¸º patches: [B, 16, 16, 16, 16]  (patch_size=16)
    â†“
å¯¹æ¯ä¸ª patch å–å¹³å‡å€¼: [B, 16, 16]
    â†“
é˜ˆå€¼åŒ– (>0.5): [B, 16, 16]
    â†“
å±•å¹³: [B, 256]
```

### 2. Attention Mask åº”ç”¨

```
Attention scores: [B, num_heads, N, N]
    â†“
Apply mask: mask_expanded = [B, 1, 1, N]
    â†“
Masked fill: attn.masked_fill(mask == 0, -inf)
    â†“
Softmax: padding positions â†’ 0
    â†“
Output: padding patches ä¸å‚ä¸ attention
```

### 3. è¾“å‡º Mask åº”ç”¨

```
Output patches: [B, num_patches, patch_dim]
    â†“
Patch mask: [B, num_patches]
    â†“
Expand: [B, num_patches, 1]
    â†“
Multiply: output * mask
    â†“
Result: padding patches â†’ 0
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

### 1. Mask æ ¼å¼

- **å›¾åƒ mask**: `[B, H, W]`ï¼Œå€¼ä¸º 1ï¼ˆæœ‰æ•ˆï¼‰æˆ– 0ï¼ˆpaddingï¼‰
- **Patch mask**: `[B, num_patches]`ï¼Œå€¼ä¸º 1ï¼ˆæœ‰æ•ˆï¼‰æˆ– 0ï¼ˆpaddingï¼‰

### 2. Condition å¤„ç†

- å¦‚æœæœ‰ conditionï¼Œpatch mask éœ€è¦æ‰©å±•ä»¥åŒ…å« condition patches
- Condition patches æ€»æ˜¯æœ‰æ•ˆçš„ï¼ˆmask = 1ï¼‰

### 3. é˜ˆå€¼é€‰æ‹©

- å½“å‰ä½¿ç”¨é˜ˆå€¼ 0.5 æ¥åˆ¤æ–­ patch æ˜¯å¦æœ‰æ•ˆ
- å¦‚æœ patch ä¸­ >50% çš„åƒç´ æ˜¯æœ‰æ•ˆçš„ï¼Œåˆ™è®¤ä¸º patch æ˜¯æœ‰æ•ˆçš„
- å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´é˜ˆå€¼

---

## ğŸ¯ æ€»ç»“

### æ”¯æŒçš„åœºæ™¯

1. âœ… **è®­ç»ƒæ—¶**ï¼šä½¿ç”¨ mask åœ¨ loss ä¸­å¿½ç•¥ padding
2. âœ… **Attention ä¸­**ï¼šä½¿ç”¨ mask é¿å…å…³æ³¨ padding åŒºåŸŸ
3. âœ… **è¾“å‡ºæ—¶**ï¼šä½¿ç”¨ mask å°† padding patches ç½®é›¶
4. âœ… **ç”Ÿæˆæ—¶**ï¼šå¯ä»¥ä½¿ç”¨ mask æ§åˆ¶ç”ŸæˆåŒºåŸŸ

### å…³é”®æ”¹è¿›

1. âœ… æ·»åŠ  `image_mask_to_patch_mask` æ–¹æ³•
2. âœ… `MultiHeadAttention` æ”¯æŒ `attention_mask` å‚æ•°
3. âœ… `TransformerBlock` ä¼ é€’ `attention_mask`
4. âœ… `forward` æ–¹æ³•æ”¯æŒ `mask` å‚æ•°
5. âœ… è¾“å‡ºæ—¶å°† padding patches ç½®é›¶

---

**ç»“è®º**ï¼š`model_jit.py` ç°åœ¨å®Œå…¨æ”¯æŒ padding maskï¼Œå¯ä»¥åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶æ­£ç¡®å¤„ç† padding åŒºåŸŸï¼Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ï¼
