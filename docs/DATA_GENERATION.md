# é¢„è®­ç»ƒæ•°æ®ç”Ÿæˆè¯´æ˜

## ğŸ¯ æ•°æ®ç”Ÿæˆé…ç½®

### æºæ•°æ®
- **ç›®å½•**: `/root/data/AI/creator/data/raw`
- **å­ç›®å½•**: `å°è¯´_1`, `å°è¯´_2`, `å°è¯´_3`, `å°è¯´_4`
- **æ¯ä¸ªç›®å½•é‡‡æ ·**: 2500 æœ¬å°è¯´

### æ•°æ®åˆ†æ®µç­–ç•¥

- **å›¾åƒå°ºå¯¸**: 64Ã—64 = 4096 tokens
- **Token é•¿åº¦èŒƒå›´**: 256-4096 tokensï¼ˆéšæœºï¼‰
- **åˆ†æ®µæ–¹å¼**: æŒ‰ç…§éšæœº token é•¿åº¦åˆ†æ®µï¼Œæ¨¡æ‹ŸçœŸå® LLM è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§

### æ•°æ®é›†åˆ’åˆ†

- **è®­ç»ƒé›†**: 99%
- **éªŒè¯é›†**: 1%

---

## ğŸ“Š æ•°æ®æ ¼å¼

### æ ·æœ¬æ ¼å¼

```json
{
  "text": "æ–‡æœ¬å†…å®¹..."
}
```

### è¾“å‡ºç›®å½•ç»“æ„

```
data/
â”œâ”€â”€ train/          # è®­ç»ƒé›†ï¼ˆ99%ï¼‰
â”‚   â”œâ”€â”€ train_0000.json
â”‚   â”œâ”€â”€ train_0001.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ val/            # éªŒè¯é›†ï¼ˆ1%ï¼‰
â”‚   â”œâ”€â”€ val_0000.json
â”‚   â””â”€â”€ ...
â””â”€â”€ stats.json      # ç»Ÿè®¡ä¿¡æ¯
```

### ç»Ÿè®¡ä¿¡æ¯æ ¼å¼

```json
{
  "total_samples": 48073,
  "train_samples": 47593,
  "val_samples": 480,
  "max_tokens": 4096,
  "min_tokens": 256,
  "img_size": 64,
  "samples_per_dir": 2500,
  "source_dirs": [...]
}
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### ç”Ÿæˆæ•°æ®

```bash
cd /root/data/AI/dlm/jit
source $(conda info --base)/etc/profile.d/conda.sh
conda activate seeme

# åå°è¿è¡Œï¼ˆæ¨èï¼‰
nohup python build_pretrain_data.py > build_pretrain_data.log 2>&1 &

# æˆ–è€…å‰å°è¿è¡Œ
python build_pretrain_data.py
```

### æ£€æŸ¥è¿›åº¦

```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f build_pretrain_data.log

# æŸ¥çœ‹è¿›ç¨‹çŠ¶æ€
ps aux | grep build_pretrain_data.py | grep -v grep

# æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶æ•°é‡
find data/train data/val -name "*.json" 2>/dev/null | wc -l

# æŸ¥çœ‹å·²ç”Ÿæˆçš„æ–‡ä»¶
ls -lh data/train/ | head -10
ls -lh data/val/ | head -10

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”Ÿæˆå®Œæˆåï¼‰
cat data/stats.json
```

### é¢„è®¡æ—¶é—´

- **å¤„ç†é€Ÿåº¦**: ~1-2 æœ¬å°è¯´/ç§’
- **æ¯ä¸ªç›®å½•**: 2500 æœ¬å°è¯´ï¼Œçº¦ 20-40 åˆ†é’Ÿ
- **æ€»æ—¶é—´**: 4 ä¸ªç›®å½•ï¼Œçº¦ 1.5-3 å°æ—¶
- **æ–‡ä»¶ä¿å­˜**: å¤„ç†å®Œæ‰€æœ‰å°è¯´åç»Ÿä¸€ä¿å­˜

---

## ğŸ“ˆ é¢„æœŸç»“æœ

### æ•°æ®é‡ä¼°ç®—

- **æ¯ä¸ªç›®å½•**: 2500 æœ¬å°è¯´
- **æ€»å°è¯´æ•°**: 10,000 æœ¬
- **æ¯æœ¬å°è¯´å¹³å‡åˆ†æ®µæ•°**: ~5-10 æ®µï¼ˆå–å†³äºå°è¯´é•¿åº¦ï¼‰
- **æ€»æ ·æœ¬æ•°**: ~50,000-100,000 ä¸ªæ ·æœ¬

### Token é•¿åº¦åˆ†å¸ƒ

- **æœ€å°**: 256 tokens
- **æœ€å¤§**: 4096 tokens
- **åˆ†å¸ƒ**: éšæœºï¼ˆæ¨¡æ‹ŸçœŸå® LLM è®­ç»ƒæ•°æ®ï¼‰

---

## âœ… éªŒè¯æ•°æ®

### æ£€æŸ¥æ•°æ®æ ¼å¼

```python
import json

# æ£€æŸ¥è®­ç»ƒé›†
with open('data/train/train_0000.json', 'r', encoding='utf-8') as f:
    train_data = json.load(f)
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}")
    print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬: {train_data[0]['text'][:100]}...")

# æ£€æŸ¥éªŒè¯é›†
with open('data/val/val_0000.json', 'r', encoding='utf-8') as f:
    val_data = json.load(f)
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_data)}")
```

### ä½¿ç”¨æ•°æ®é›†

```python
from dataset import TokenImageDataset
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('/root/data/AI/pretrain/Qwen2.5-7B-Instruct', trust_remote_code=True)

# åŠ è½½è®­ç»ƒé›†
train_dataset = TokenImageDataset(
    data_path='./data/train',
    tokenizer=tokenizer,
    img_size=64,
)

# åŠ è½½éªŒè¯é›†
val_dataset = TokenImageDataset(
    data_path='./data/val',
    tokenizer=tokenizer,
    img_size=64,
)

print(f"è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
print(f"éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
```

---

## ğŸ” æ³¨æ„äº‹é¡¹

1. **Token é•¿åº¦**: æ¯ä¸ªæ ·æœ¬çš„ token é•¿åº¦æ˜¯éšæœºçš„ï¼ˆ256-4096ï¼‰ï¼Œä½¿ç”¨ padding mask æ ‡è®°æœ‰æ•ˆåŒºåŸŸ
2. **æ–‡ä»¶å¤§å°**: æ¯ä¸ª JSON æ–‡ä»¶åŒ…å«çº¦ 10,000 ä¸ªæ ·æœ¬ï¼Œé¿å…å•ä¸ªæ–‡ä»¶è¿‡å¤§
3. **ç¼–ç é—®é¢˜**: è„šæœ¬ä¼šè‡ªåŠ¨å°è¯•å¤šç§ç¼–ç ï¼ˆutf-8, gbk, gb2312, gb18030ï¼‰æ¥è¯»å–æ–‡ä»¶
4. **å¤„ç†æ—¶é—´**: å¤„ç† 10,000 æœ¬å°è¯´å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®åœ¨åå°è¿è¡Œ

---

## ğŸ“ æ—¥å¿—æ–‡ä»¶

æ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„æ—¥å¿—ä¿å­˜åœ¨ `build_pretrain_data.log`ï¼ŒåŒ…å«ï¼š
- å¤„ç†è¿›åº¦
- æ¯ä¸ªç›®å½•çš„æ ·æœ¬æ•°
- é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯
- æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯

---

**ç”Ÿæˆæ—¥æœŸ**: 2024-12-15
**å›¾åƒå°ºå¯¸**: 64Ã—64
**Token èŒƒå›´**: 256-4096 tokens
