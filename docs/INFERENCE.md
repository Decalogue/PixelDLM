# æ¨ç†æŒ‡å—

## ğŸ¯ æ¨ç†æµç¨‹

### æ— æ¡ä»¶ç”Ÿæˆ

```python
from model import build_jit_model
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹
model = build_jit_model('JiT-B/4', img_size=64)
tokenizer = AutoTokenizer.from_pretrained('Qwen2.5-7B-Instruct')

# æ— æ¡ä»¶ç”Ÿæˆ
text = model.generate(
    num_steps=20,
    guidance_scale=1.0,
    condition=None
)
```

### æ¡ä»¶ç”Ÿæˆ

```python
# æ¡ä»¶ç”Ÿæˆ
text = model.generate(
    prompt="What is AI?",
    num_steps=20,
    guidance_scale=2.0
)
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### ä½¿ç”¨ infer.shï¼ˆæ¨èï¼‰

```bash
# ç¼–è¾‘ infer.sh é…ç½®å‚æ•°
CHECKPOINT="./run/jit_v1/best_model.pth"
MODEL="JiT-B/4"
IMG_SIZE=64
NUM_INFERENCE_STEPS=20

# è¿è¡Œæ¨ç†
./infer.sh
```

### ä½¿ç”¨ Python è„šæœ¬

```bash
python inference.py \
    --checkpoint ./run/jit_v1/best_model.pth \
    --model JiT-B/4 \
    --img_size 64 \
    --num_inference_steps 20 \
    --save_image
```

## ğŸ“Š è¯„ä¼°æ¨¡å‹

```bash
python evaluate.py \
    --checkpoint ./run/jit_v1/best_model.pth \
    --data_path ./data/val \
    --num_samples 100
```

## âœ… å…³é”®ç‰¹æ€§

1. **æ— æ¡ä»¶ç”Ÿæˆ**ï¼šç›´æ¥ä»å™ªå£°ç”Ÿæˆæ–‡æœ¬å›¾åƒ
2. **æ¡ä»¶ç”Ÿæˆ**ï¼šåŸºäº prompt ç”Ÿæˆ answer
3. **Padding Mask æ”¯æŒ**ï¼šåªå¯¹æœ‰æ•ˆåƒç´ è®¡ç®—æŒ‡æ ‡
4. **DDIM é‡‡æ ·**ï¼š20 æ­¥å¿«é€Ÿé‡‡æ ·

---

**æ›´æ–°æ—¥æœŸ**: 2025-12-15
