# Dataset è®¾è®¡è¯´æ˜

## ğŸ¯ è®¾è®¡ç†å¿µ

### æ ¸å¿ƒè§‚ç‚¹

**é¢„è®­ç»ƒæ—¶åªéœ€è¦çº¯æ–‡æœ¬ï¼Œä¸éœ€è¦é—®ç­”å¯¹æ ¼å¼ï¼**

åŸå› ï¼š
1. **Base å’Œ Chat ç‰ˆæœ¬çš„åŒºåˆ«ä¸»è¦åœ¨äº `chat_template`**ï¼ˆæ¨ç†æ—¶åº”ç”¨ï¼‰
2. **é¢„è®­ç»ƒ**ï¼šæ¨¡å‹å­¦ä¹ ä»å™ªå£°å›¾åƒæ¢å¤åŸå§‹å›¾åƒï¼ˆè‡ªç›‘ç£å­¦ä¹ ï¼‰
3. **å¾®è°ƒ**ï¼šå¯ä»¥é€šè¿‡ `chat_template` æ ¼å¼åŒ–è¾“å…¥ï¼Œå®ç°æ¡ä»¶ç”Ÿæˆ

---

## ğŸ“Š è®¾è®¡å¯¹æ¯”

### æ—§è®¾è®¡ï¼ˆé—®ç­”å¯¹æ ¼å¼ï¼‰âŒ

```python
# é—®é¢˜åŒºåŸŸ + ç­”æ¡ˆåŒºåŸŸ
question_size = (32, 32)  # é—®é¢˜åŒºåŸŸ
answer_size = (224, 256)   # ç­”æ¡ˆåŒºåŸŸ
full_image = question + answer  # æ‹¼æ¥

# é—®é¢˜ï¼š
1. æµªè´¹å›¾åƒç©ºé—´ï¼ˆé—®é¢˜åŒºåŸŸå›ºå®šï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ï¼‰
2. é¢„è®­ç»ƒæ—¶ä¸éœ€è¦é—®ç­”å¯¹æ ¼å¼
3. ä¸ç¬¦åˆå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ–¹å¼
```

### æ–°è®¾è®¡ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰âœ…

```python
# æ•´ä¸ªå›¾åƒéƒ½æ˜¯æ–‡æœ¬
img_size = 256  # æ•´ä¸ªå›¾åƒç”¨äºæ–‡æœ¬

# ä¼˜åŠ¿ï¼š
1. å……åˆ†åˆ©ç”¨å›¾åƒç©ºé—´ï¼ˆ256Ã—256 = 65,536 tokensï¼‰
2. é¢„è®­ç»ƒæ—¶åªéœ€è¦çº¯æ–‡æœ¬
3. ç¬¦åˆå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ–¹å¼
4. å¾®è°ƒæ—¶å¯ä»¥é€šè¿‡ chat_template æ ¼å¼åŒ–
```

---

## ğŸ”§ å®ç°ç»†èŠ‚

### 1. æ•°æ®æ ¼å¼æ”¯æŒ

#### çº¯æ–‡æœ¬æ ¼å¼ï¼ˆé¢„è®­ç»ƒç”¨ï¼‰

```json
[
  {"text": "The capital of France is Paris..."},
  {"text": "Quantum computing uses quantum mechanical..."},
  ...
]
```

#### é—®ç­”å¯¹æ ¼å¼ï¼ˆå¾®è°ƒç”¨ï¼Œå¯é€‰ï¼‰

```json
[
  {"question": "What is the capital of France?", "answer": "Paris"},
  ...
]
```

å¦‚æœ `use_chat_template=True`ï¼Œä¼šè‡ªåŠ¨æ ¼å¼åŒ–ä¸ºå¯¹è¯æ ¼å¼ã€‚

#### æ–‡æœ¬æ–‡ä»¶æ ¼å¼

```
æ¯è¡Œä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µ
The capital of France is Paris...
Quantum computing uses quantum mechanical...
...
```

### 2. ç¼–ç æµç¨‹

```python
# 1. åŠ è½½æ–‡æœ¬
text = "The capital of France is Paris..."

# 2. Tokenizeï¼ˆæ”¯æŒåˆ†æ®µå¤„ç†ï¼Œé¿å…è¶…è¿‡ Tokenizer æœ€å¤§é•¿åº¦ï¼‰
token_ids = tokenizer.encode(text, add_special_tokens=False)

# 3. ç¼–ç åˆ°å›¾åƒï¼ˆ256Ã—256ï¼‰
img = encode_token_ids_to_image(token_ids, size=(256, 256))

# 4. è®­ç»ƒï¼šä»å™ªå£°å›¾åƒæ¢å¤åŸå§‹å›¾åƒ
noisy_img = add_noise(clean_img, timestep)
predicted_img = model(noisy_img, timestep, condition=None)
loss = mse_loss(predicted_img, clean_img)
```

### 3. è®­ç»ƒæ¨¡å¼

#### é¢„è®­ç»ƒï¼ˆæ— æ¡ä»¶ï¼‰

```python
# æ¨¡å‹å­¦ä¹ ä»å™ªå£°æ¢å¤åŸå§‹å›¾åƒ
clean_pred = model(noisy_img, timestep, condition=None)
```

#### å¾®è°ƒï¼ˆæ¡ä»¶ç”Ÿæˆï¼Œå¯é€‰ï¼‰

```python
# ä½¿ç”¨ chat_template æ ¼å¼åŒ–è¾“å…¥
messages = [
    {"role": "user", "content": "What is the capital of France?"}
]
formatted_text = tokenizer.apply_chat_template(messages, tokenize=False)

# ç¼–ç ä¸ºæ¡ä»¶å›¾åƒ
condition_img = encode_text_to_image(formatted_text, size=(32, 32))

# æ¡ä»¶ç”Ÿæˆ
clean_pred = model(noisy_img, timestep, condition=condition_img)
```

---

## ğŸ“ˆ ä½¿ç”¨ç¤ºä¾‹

### é¢„è®­ç»ƒæ•°æ®å‡†å¤‡

```python
from dataset import TokenImageDataset
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('Qwen2.5-7B-Instruct')

# çº¯æ–‡æœ¬æ•°æ®
dataset = TokenImageDataset(
    data_path='./data/pretrain_texts.json',  # æˆ–æ–‡æœ¬æ–‡ä»¶ç›®å½•
    tokenizer=tokenizer,
    img_size=256,
    use_chat_template=False,  # é¢„è®­ç»ƒä¸éœ€è¦ chat_template
)

# è®­ç»ƒ
for batch in dataloader:
    clean = batch['clean']  # [B, 3, 256, 256]
    # æ·»åŠ å™ªå£°å¹¶è®­ç»ƒ...
```

### å¾®è°ƒæ•°æ®å‡†å¤‡ï¼ˆå¯é€‰ï¼‰

```python
# é—®ç­”å¯¹æ•°æ®
dataset = TokenImageDataset(
    data_path='./data/qa_pairs.json',
    tokenizer=tokenizer,
    img_size=256,
    use_chat_template=True,  # ä½¿ç”¨ chat_template æ ¼å¼åŒ–
)

# è®­ç»ƒæ—¶å¯ä»¥ä½¿ç”¨æ¡ä»¶ç”Ÿæˆ
for batch in dataloader:
    clean = batch['clean']
    # å¦‚æœéœ€è¦æ¡ä»¶ç”Ÿæˆï¼Œå¯ä»¥ä» clean ä¸­æå–æ¡ä»¶éƒ¨åˆ†
    # æˆ–è€…å•ç‹¬å‡†å¤‡æ¡ä»¶å›¾åƒ
```

---

## ğŸ¯ å…³é”®ä¼˜åŠ¿

### 1. ç¬¦åˆå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹å¼

```
ä¼ ç»Ÿ LLM é¢„è®­ç»ƒï¼š
- è¾“å…¥ï¼šçº¯æ–‡æœ¬
- ç›®æ ‡ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token

æˆ‘ä»¬çš„æ–¹æ¡ˆï¼š
- è¾“å…¥ï¼šæ–‡æœ¬ç¼–ç çš„å›¾åƒ
- ç›®æ ‡ï¼šä»å™ªå£°å›¾åƒæ¢å¤åŸå§‹å›¾åƒ
- æœ¬è´¨ï¼šè‡ªç›‘ç£å­¦ä¹ 
```

### 2. å……åˆ†åˆ©ç”¨å›¾åƒç©ºé—´

```
æ—§è®¾è®¡ï¼š
- é—®é¢˜åŒºåŸŸï¼š32Ã—32 = 1,024 tokens
- ç­”æ¡ˆåŒºåŸŸï¼š224Ã—256 = 57,344 tokens
- æ€»å®¹é‡ï¼š58,368 tokens

æ–°è®¾è®¡ï¼š
- æ•´ä¸ªå›¾åƒï¼š256Ã—256 = 65,536 tokens
- åˆ©ç”¨ç‡ï¼š100%
```

### 3. çµæ´»çš„æ•°æ®æ ¼å¼

```
æ”¯æŒï¼š
- JSON æ ¼å¼ï¼ˆçº¯æ–‡æœ¬æˆ–é—®ç­”å¯¹ï¼‰
- æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µï¼‰
- ç›®å½•ï¼ˆåŒ…å«å¤šä¸ªæ–‡ä»¶ï¼‰
```

### 4. é¢„è®­ç»ƒå’Œå¾®è°ƒç»Ÿä¸€

```
é¢„è®­ç»ƒï¼š
- çº¯æ–‡æœ¬ â†’ å›¾åƒ
- æ— æ¡ä»¶ç”Ÿæˆ

å¾®è°ƒï¼š
- é—®ç­”å¯¹ â†’ chat_template â†’ å›¾åƒ
- æ¡ä»¶ç”Ÿæˆï¼ˆå¯é€‰ï¼‰
```

---

## ğŸ” ä¸ Base/Chat æ¨¡å‹çš„å…³ç³»

### Base æ¨¡å‹

```
é¢„è®­ç»ƒï¼š
- çº¯æ–‡æœ¬æ•°æ®
- å­¦ä¹ è¯­è¨€è¡¨ç¤º

æ¨ç†ï¼š
- ç›´æ¥ä½¿ç”¨ tokenizer
- æ—  chat_template
```

### Chat æ¨¡å‹

```
é¢„è®­ç»ƒï¼š
- åŒæ ·ä½¿ç”¨çº¯æ–‡æœ¬æ•°æ®ï¼ˆä¸ Base ç›¸åŒï¼‰

å¾®è°ƒï¼š
- ä½¿ç”¨ chat_template æ ¼å¼åŒ–å¯¹è¯
- å­¦ä¹ å¯¹è¯æ ¼å¼

æ¨ç†ï¼š
- ä½¿ç”¨ chat_template æ ¼å¼åŒ–è¾“å…¥
- ç”Ÿæˆç¬¦åˆå¯¹è¯æ ¼å¼çš„è¾“å‡º
```

**å…³é”®**ï¼šBase å’Œ Chat çš„åŒºåˆ«ä¸»è¦åœ¨äº `chat_template`ï¼Œè€Œä¸æ˜¯é¢„è®­ç»ƒæ•°æ®æ ¼å¼ï¼

---

## âœ… æ€»ç»“

### æ–°è®¾è®¡çš„ä¼˜åŠ¿

1. âœ… **ç¬¦åˆå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹å¼**ï¼šé¢„è®­ç»ƒæ—¶åªéœ€è¦çº¯æ–‡æœ¬
2. âœ… **å……åˆ†åˆ©ç”¨å›¾åƒç©ºé—´**ï¼šæ•´ä¸ª 256Ã—256 å›¾åƒç”¨äºæ–‡æœ¬
3. âœ… **çµæ´»çš„æ•°æ®æ ¼å¼**ï¼šæ”¯æŒå¤šç§æ•°æ®æ ¼å¼
4. âœ… **é¢„è®­ç»ƒå’Œå¾®è°ƒç»Ÿä¸€**ï¼šåŒä¸€å¥—ä»£ç ï¼Œä¸åŒé…ç½®

### å…³é”®æ”¹è¿›

- âŒ ç§»é™¤ `question_size`ï¼šä¸å†éœ€è¦å›ºå®šçš„é—®é¢˜åŒºåŸŸ
- âŒ ç§»é™¤ `answer_size`ï¼šæ•´ä¸ªå›¾åƒéƒ½æ˜¯æ–‡æœ¬
- âœ… æ·»åŠ  `use_chat_template`ï¼šå¾®è°ƒæ—¶å¯é€‰
- âœ… æ”¯æŒçº¯æ–‡æœ¬æ•°æ®ï¼šé¢„è®­ç»ƒç”¨

### ä½¿ç”¨å»ºè®®

1. **é¢„è®­ç»ƒ**ï¼šä½¿ç”¨çº¯æ–‡æœ¬æ•°æ®ï¼Œ`use_chat_template=False`
2. **å¾®è°ƒ**ï¼šä½¿ç”¨é—®ç­”å¯¹æ•°æ®ï¼Œ`use_chat_template=True`ï¼ˆå¯é€‰ï¼‰
3. **æ¨ç†**ï¼šä½¿ç”¨ `chat_template` æ ¼å¼åŒ–è¾“å…¥ï¼Œå®ç°æ¡ä»¶ç”Ÿæˆ

---

**ç»“è®º**ï¼šæ–°è®¾è®¡æ›´ç¬¦åˆå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ–¹å¼ï¼Œé¢„è®­ç»ƒæ—¶åªéœ€è¦çº¯æ–‡æœ¬ï¼Œä¸éœ€è¦é—®ç­”å¯¹æ ¼å¼ï¼
