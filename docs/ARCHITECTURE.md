# 模型架构设计

## 🏗️ 核心架构

### JiT-B/4 模型配置

| 配置项 | 值 |
|--------|-----|
| **参数量** | ~128M (0.128B) |
| **图像尺寸** | 64×64 |
| **Patch Size** | 4×4 |
| **Patches 数量** | 256 |
| **Embed Dim** | 768 |
| **Depth** | 12 层 |
| **Num Heads** | 12 |
| **序列长度** | 512 (条件 256 + 目标 256) |

### 统一序列架构

**核心设计**：无论是否有条件，都使用**固定的最大序列长度**（512）

```
总序列长度：512 patches（固定）

无条件生成:
[PAD × 256 | Target × 256]  ← 前 256 个位置为 padding

条件生成:
[Condition × 256 | Target × 256]  ← 前 256 个位置为条件
```

**优势**：
- ✅ 预训练和微调架构完全一致
- ✅ 无需重新训练，直接支持条件生成
- ✅ 使用 attention mask 控制计算

### Transformer 架构

```
输入: 64×64 图像 (RGB)
  ↓
Patch Embedding (4×4 patches → 256 patches)
  ↓
统一序列架构 (固定长度 512)
  ├─ Condition (0-256) 或 Padding
  └─ Target (256-512)
  ↓
Transformer Blocks × 12
  ├─ Multi-Head Attention (12 heads)
  ├─ SwiGLU MLP (MLP ratio=4)
  └─ AdaLNZero (时间条件归一化)
  ↓
Output Projection
  ↓
输出: 预测的干净图像 patches
```

## 🎯 模型变体

### Base 模型

| 模型 | patch_size | 适合图像尺寸 | num_patches (64×64) |
|------|------------|--------------|---------------------|
| **JiT-B/4** | 4 | 64×64 | 256 |
| JiT-B/8 | 8 | 64×64, 128×128 | 64 |
| JiT-B/16 | 16 | 256×256 | 16 |

### Large / Huge 模型

| 模型 | embed_dim | depth | num_heads |
|------|-----------|-------|-----------|
| JiT-L/16 | 1024 | 24 | 16 |
| JiT-H/16 | 1280 | 32 | 16 |

## 📊 Attention Mask 机制

```python
# 无条件：mask 掉前 256 个位置
mask_uncond = [0, 0, ..., 0, | 1, 1, ..., 1]
               ↑ 256 个 0    ↑ 256 个 1

# 条件：前 256 个位置有效
mask_cond = [1, 1, ..., 1, | 1, 1, ..., 1]
            ↑ 256 个 1     ↑ 256 个 1
```

## ✅ 设计优势

1. **架构完全统一**：预训练和微调使用相同的序列长度（512）
2. **训练一致性**：模型从一开始就适应 512 长度的序列
3. **实现简单**：只需修改 forward 方法，使用 attention mask 控制计算

---

**更新日期**: 2025-12-15
