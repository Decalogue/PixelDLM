# æ¡ä»¶ä½ç½®ç¼–ç ä¿®å¤æ–¹æ¡ˆ

## ğŸš¨ é—®é¢˜åˆ†æ

### å½“å‰ä»£ç çš„é—®é¢˜

```python
# ä½ç½®ç¼–ç ï¼ˆåªé’ˆå¯¹ç›®æ ‡å›¾åƒï¼‰
x = self.patch_embed(x_patches)  # [B, num_patches, embed_dim]
x = x + self.pos_embed  # pos_embed: [1, num_patches, embed_dim]

# æ¡ä»¶åµŒå…¥
if condition is not None:
    cond_embed = self.condition_embed(condition)  # [B, cond_patches, embed_dim]
    x = torch.cat([cond_embed, x], dim=1)  # [B, cond_patches + num_patches, embed_dim]
```

**é—®é¢˜**ï¼š
1. âŒ `pos_embed` åªé’ˆå¯¹ `num_patches`ï¼Œæ¡ä»¶éƒ¨åˆ†æ²¡æœ‰ä½ç½®ç¼–ç 
2. âŒ æ‹¼æ¥ååºåˆ—é•¿åº¦å˜åŒ–ï¼Œä½†ä½ç½®ç¼–ç ä¸åŒ¹é…
3. âŒ æ¡ä»¶éƒ¨åˆ†çš„ä½ç½®ä¿¡æ¯ä¸¢å¤±

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä¸ºæ¡ä»¶æ·»åŠ ç‹¬ç«‹çš„ä½ç½®ç¼–ç ï¼ˆæ¨èï¼‰â­

**è®¾è®¡**ï¼š
- æ¡ä»¶ä½¿ç”¨ç‹¬ç«‹çš„ä½ç½®ç¼–ç ï¼ˆå¯å­¦ä¹ æˆ–å›ºå®šï¼‰
- ç›®æ ‡å›¾åƒä½¿ç”¨åŸæœ‰çš„ä½ç½®ç¼–ç 

**å®ç°**ï¼š

```python
class JiT(nn.Module):
    def __init__(self, ...):
        # åŸæœ‰ä½ç½®ç¼–ç ï¼ˆç›®æ ‡å›¾åƒï¼‰
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        
        # æ¡ä»¶ä½ç½®ç¼–ç ï¼ˆå¯å­¦ä¹ ï¼‰
        # å‡è®¾æ¡ä»¶å›¾åƒæ˜¯ 32Ã—32ï¼Œpatch_size=4ï¼Œåˆ™ cond_patches = (32//4)^2 = 64
        self.cond_pos_embed = nn.Parameter(torch.zeros(1, 64, embed_dim))  # å¯é…ç½®
        
        # æˆ–è€…ä½¿ç”¨å›ºå®šä½ç½®ç¼–ç 
        # self.cond_pos_embed = self._create_cond_pos_embed(max_cond_patches=64)
    
    def forward(self, x, t, condition=None, mask=None):
        # ç›®æ ‡å›¾åƒåµŒå…¥å’Œä½ç½®ç¼–ç 
        x = self.patch_embed(x_patches)  # [B, num_patches, embed_dim]
        x = x + self.pos_embed  # [B, num_patches, embed_dim]
        
        # æ¡ä»¶åµŒå…¥å’Œä½ç½®ç¼–ç 
        if condition is not None:
            cond_embed = self.condition_embed(condition)  # [B, cond_patches, embed_dim]
            # ä¸ºæ¡ä»¶æ·»åŠ ä½ç½®ç¼–ç 
            cond_pos = self.cond_pos_embed[:, :cond_embed.shape[1], :]  # æˆªå–æˆ–å¡«å……
            cond_embed = cond_embed + cond_pos
            # æ‹¼æ¥
            x = torch.cat([cond_embed, x], dim=1)  # [B, cond_patches + num_patches, embed_dim]
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ¡ä»¶æœ‰ç‹¬ç«‹çš„ä½ç½®ä¿¡æ¯
- âœ… å®ç°ç®€å•
- âœ… å¯å­¦ä¹ çš„ä½ç½®ç¼–ç æ›´çµæ´»

---

### æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆæ›´çµæ´»ï¼‰

**è®¾è®¡**ï¼š
- ä½¿ç”¨ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œè‡ªåŠ¨é€‚åº”åºåˆ—é•¿åº¦
- æ¡ä»¶å’Œç›®æ ‡ä½¿ç”¨ä¸åŒçš„ä½ç½®ç¼–ç ç©ºé—´

**å®ç°**ï¼š

```python
class JiT(nn.Module):
    def __init__(self, ...):
        # ä½¿ç”¨å¯å­¦ä¹ çš„ç›¸å¯¹ä½ç½®ç¼–ç 
        self.max_seq_len = 1024  # è¶³å¤Ÿå¤§çš„åºåˆ—é•¿åº¦
        self.pos_embed = nn.Parameter(torch.zeros(1, self.max_seq_len, embed_dim))
        
    def forward(self, x, t, condition=None, mask=None):
        # ç›®æ ‡å›¾åƒ
        x = self.patch_embed(x_patches)
        x = x + self.pos_embed[:, :x.shape[1], :]  # åŠ¨æ€æˆªå–
        
        # æ¡ä»¶
        if condition is not None:
            cond_embed = self.condition_embed(condition)
            # æ¡ä»¶ä½¿ç”¨ä¸åŒçš„ä½ç½®ç¼–ç èŒƒå›´
            cond_start_idx = 0
            cond_embed = cond_embed + self.pos_embed[:, cond_start_idx:cond_start_idx+cond_embed.shape[1], :]
            x = torch.cat([cond_embed, x], dim=1)
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ›´çµæ´»ï¼Œé€‚åº”ä¸åŒé•¿åº¦çš„æ¡ä»¶
- âœ… æ¡ä»¶ä¸ç›®æ ‡ä½¿ç”¨ä¸åŒçš„ä½ç½®ç©ºé—´

---

### æ–¹æ¡ˆ 3ï¼šæ¡ä»¶ä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆç®€å•ä½†ä¸æ¨èï¼‰

**è®¾è®¡**ï¼š
- æ¡ä»¶ä½œä¸º"å…¨å±€ä¸Šä¸‹æ–‡"ï¼Œä¸éœ€è¦ä½ç½®ä¿¡æ¯
- åªå¯¹ç›®æ ‡å›¾åƒä½¿ç”¨ä½ç½®ç¼–ç 

**å®ç°**ï¼š

```python
if condition is not None:
    cond_embed = self.condition_embed(condition)
    # æ¡ä»¶ä¸åŠ ä½ç½®ç¼–ç ï¼ˆä½œä¸ºå…¨å±€ä¸Šä¸‹æ–‡ï¼‰
    x = self.patch_embed(x_patches)
    x = x + self.pos_embed  # åªå¯¹ç›®æ ‡å›¾åƒåŠ ä½ç½®ç¼–ç 
    x = torch.cat([cond_embed, x], dim=1)
```

**ç¼ºç‚¹**ï¼š
- âŒ æ¡ä»¶å¤±å»äº†ä½ç½®ä¿¡æ¯
- âŒ å¦‚æœæ¡ä»¶è¾ƒé•¿ï¼Œå¯èƒ½å½±å“æ•ˆæœ

---

## ğŸ¯ æ¨èå®ç°ï¼ˆæ–¹æ¡ˆ 1ï¼‰

### å®Œæ•´ä¿®å¤ä»£ç 

```python
class JiT(nn.Module):
    def __init__(
        self,
        img_size: int = 64,
        patch_size: int = 4,
        cond_img_size: int = 32,  # æ–°å¢ï¼šæ¡ä»¶å›¾åƒå°ºå¯¸
        ...
    ):
        # åŸæœ‰ä»£ç 
        self.num_patches = (img_size // patch_size) ** 2
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        
        # æ–°å¢ï¼šæ¡ä»¶ä½ç½®ç¼–ç 
        self.cond_num_patches = (cond_img_size // patch_size) ** 2
        self.cond_pos_embed = nn.Parameter(torch.zeros(1, self.cond_num_patches, embed_dim))
        
        # åˆå§‹åŒ–
        nn.init.normal_(self.pos_embed, std=0.02)
        nn.init.normal_(self.cond_pos_embed, std=0.02)  # æ¡ä»¶ä½ç½®ç¼–ç 
    
    def forward(self, x, t, condition=None, mask=None):
        # ç›®æ ‡å›¾åƒåµŒå…¥å’Œä½ç½®ç¼–ç 
        x = self.patch_embed(x_patches)  # [B, num_patches, embed_dim]
        x = x + self.pos_embed  # [B, num_patches, embed_dim]
        
        # æ¡ä»¶åµŒå…¥å’Œä½ç½®ç¼–ç 
        if condition is not None:
            cond_embed = self.condition_embed(condition)  # [B, cond_patches, embed_dim]
            
            # ä¸ºæ¡ä»¶æ·»åŠ ä½ç½®ç¼–ç 
            # å¦‚æœ cond_patches å°äº cond_num_patchesï¼Œæˆªå–
            # å¦‚æœ cond_patches å¤§äº cond_num_patchesï¼Œéœ€è¦å¤„ç†ï¼ˆé€šå¸¸ä¸ä¼šï¼‰
            cond_pos = self.cond_pos_embed[:, :cond_embed.shape[1], :]
            cond_embed = cond_embed + cond_pos
            
            # æ‹¼æ¥
            x = torch.cat([cond_embed, x], dim=1)  # [B, cond_patches + num_patches, embed_dim]
        
        # Transformer blocks
        for block in self.blocks:
            x = block(x, t_embed, attention_mask=patch_mask)
        
        # æå–ç›®æ ‡éƒ¨åˆ†ï¼ˆç§»é™¤æ¡ä»¶ï¼‰
        if condition is not None:
            x = x[:, condition.shape[1]:, :]
        
        return x
```

---

## ğŸ“Š ç»´åº¦å˜åŒ–åˆ†æ

### æ— æ¡ä»¶ç”Ÿæˆ

```
è¾“å…¥: x_patches [B, 256, 48]  # 64Ã—64, patch_size=4 â†’ 256 patches
  â†“ patch_embed
x: [B, 256, 768]
  â†“ + pos_embed [1, 256, 768]
x: [B, 256, 768]
  â†“ Transformer
x: [B, 256, 768]
  â†“ output_proj
x: [B, 256, 48]
```

### æ¡ä»¶ç”Ÿæˆï¼ˆä¿®å¤åï¼‰

```
æ¡ä»¶: condition [B, 64, 48]  # 32Ã—32, patch_size=4 â†’ 64 patches
  â†“ condition_embed
cond_embed: [B, 64, 768]
  â†“ + cond_pos_embed [1, 64, 768]
cond_embed: [B, 64, 768]

ç›®æ ‡: x_patches [B, 256, 48]
  â†“ patch_embed
x: [B, 256, 768]
  â†“ + pos_embed [1, 256, 768]
x: [B, 256, 768]

  â†“ concat
x: [B, 320, 768]  # 64 + 256 = 320
  â†“ Transformer (å¤„ç† 320 ä¸ª tokens)
x: [B, 320, 768]
  â†“ æå–ç›®æ ‡éƒ¨åˆ†
x: [B, 256, 768]  # ç§»é™¤å‰ 64 ä¸ªï¼ˆæ¡ä»¶ï¼‰
  â†“ output_proj
x: [B, 256, 48]
```

---

## âœ… æ€»ç»“

### é—®é¢˜ç¡®è®¤

1. âœ… **ç»´åº¦ç¡®å®ä¼šå˜å¤§**ï¼š`cond_patches + num_patches`
2. âœ… **ä½ç½®ç¼–ç ä¸åŒ¹é…**ï¼šæ¡ä»¶éƒ¨åˆ†æ²¡æœ‰ä½ç½®ç¼–ç 
3. âœ… **éœ€è¦ä¿®å¤**ï¼šä¸ºæ¡ä»¶æ·»åŠ ç‹¬ç«‹çš„ä½ç½®ç¼–ç 

### æ¨èæ–¹æ¡ˆ

**æ–¹æ¡ˆ 1ï¼šç‹¬ç«‹çš„æ¡ä»¶ä½ç½®ç¼–ç **
- å®ç°ç®€å•
- æ¡ä»¶æœ‰ç‹¬ç«‹çš„ä½ç½®ä¿¡æ¯
- æ¨èä½¿ç”¨

### ä¿®å¤è¦ç‚¹

1. æ·»åŠ  `cond_pos_embed` å‚æ•°
2. åœ¨æ¡ä»¶åµŒå…¥åæ·»åŠ ä½ç½®ç¼–ç 
3. ç¡®ä¿ç»´åº¦åŒ¹é…ï¼ˆæˆªå–æˆ–å¡«å……ï¼‰

---

**ä¿®æ”¹æ—¥æœŸ**: 2025-12-15
