# è®­ç»ƒæŒ‡å—

## â±ï¸ è®­ç»ƒæ—¶é—´ä¼°ç®—

### ç¡¬ä»¶é…ç½®

- **GPU**: 8Ã— H200
- **æ˜¾å­˜**: 141GB HBM3 per GPU
- **Batch Size**: 64 per GPU (æ€» 512)
- **Mixed Precision**: BF16/FP16

### è®­ç»ƒé€Ÿåº¦ä¼°ç®—

| åœºæ™¯ | è®­ç»ƒé€Ÿåº¦ | 1T Tokens | 10B Tokens |
|------|---------|-----------|------------|
| **ä¿å®ˆä¼°è®¡** | 240K tokens/s | ~48 å¤© | ~11.6 å°æ—¶ |
| **ä¸­ç­‰ä¼°è®¡** â­ | 400K tokens/s | ~29 å¤© | ~7 å°æ—¶ |
| **ä¹è§‚ä¼°è®¡** | 640K tokens/s | ~18 å¤© | ~4.3 å°æ—¶ |

### æ¨èä¼°ç®—

**1T Tokens**: 30-60 å¤©ï¼ˆ1-2 ä¸ªæœˆï¼‰  
**10B Tokens**: 7-12 å°æ—¶ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰

## ğŸš€ è®­ç»ƒä¼˜åŒ–

### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```bash
# ä½¿ç”¨ BF16/FP16 åŠ é€Ÿ
--use_amp
```

### 2. æ¢¯åº¦ç´¯ç§¯

```bash
# å‡å°‘é€šä¿¡å¼€é”€
--gradient_accumulation_steps 2
```

### 3. å¤§ Batch Size

- **å•å¡**: 64 (å……åˆ†åˆ©ç”¨ H200 çš„ 141GB æ˜¾å­˜)
- **æ€» Batch**: 512 (8 å¡ Ã— 64)

### 4. æ•°æ®é¢„å¤„ç†

- æå‰å¤„ç†æ•°æ®ï¼Œå‡å°‘ I/O æ—¶é—´
- ä½¿ç”¨ `persistent_workers=True` ä¿æŒæ•°æ®åŠ è½½å™¨

## ğŸ“Š è®­ç»ƒæµç¨‹

### é¢„è®­ç»ƒï¼ˆæ— æ¡ä»¶ï¼‰

```bash
python train.py \
    --data_path ./data/train \
    --img_size 64 \
    --model JiT-B/4 \
    --batch_size 64 \
    --epochs 100
```

### å¾®è°ƒï¼ˆæ¡ä»¶ç”Ÿæˆï¼‰

```bash
python train.py \
    --data_path ./data/train \
    --img_size 64 \
    --model JiT-B/4 \
    --enable_condition \
    --batch_size 64 \
    --epochs 50
```

## ğŸ–¥ï¸ å¤šå¡è®­ç»ƒ

### ä½¿ç”¨æ–¹æ³•

è®­ç»ƒè„šæœ¬å·²æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼ˆDDPï¼‰ï¼Œå¯ä»¥åœ¨å¤šå¼  GPU ä¸Šå¹¶è¡Œè®­ç»ƒï¼š

```bash
# ä½¿ç”¨ 2 å¼  GPU
torchrun --nproc_per_node=2 train.py \
    --data_path ./data/train \
    --batch_size 64 \
    --epochs 100

# ä½¿ç”¨ 8 å¼  GPU
torchrun --nproc_per_node=8 train.py \
    --data_path ./data/train \
    --batch_size 64 \
    --epochs 100
```

### æ‰¹æ¬¡å¤§å°è¯´æ˜

- **`--batch_size`**: æ¯å¼  GPU çš„æ‰¹æ¬¡å¤§å°
- **æœ‰æ•ˆæ‰¹æ¬¡å¤§å°** = `batch_size Ã— GPUæ•°é‡ Ã— gradient_accumulation_steps`

**ç¤ºä¾‹**ï¼š
- å•å¡ï¼š`batch_size=128`ï¼Œæœ‰æ•ˆæ‰¹æ¬¡ = 128
- åŒå¡ï¼š`batch_size=64`ï¼Œæœ‰æ•ˆæ‰¹æ¬¡ = 128ï¼ˆä¿æŒä¸å˜ï¼‰
- åŒå¡ï¼š`batch_size=128`ï¼Œæœ‰æ•ˆæ‰¹æ¬¡ = 256ï¼ˆå¢åŠ ï¼‰

## ğŸ“ˆ WandB ç›‘æ§

### å¯ç”¨ç›‘æ§

è®­ç»ƒè„šæœ¬å·²é›†æˆ WandB ç”¨äºå®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼š

```bash
python train.py \
    --data_path ./data/train \
    --use_wandb \
    --wandb_project "jit-diffusion" \
    --wandb_name "jit-v1"
```

### ç›‘æ§æŒ‡æ ‡

è‡ªåŠ¨è®°å½•ä»¥ä¸‹æŒ‡æ ‡ï¼š
- **`train/loss`**: æ¯ä¸ªä¼˜åŒ–æ­¥éª¤çš„è®­ç»ƒæŸå¤±
- **`train/learning_rate`**: å½“å‰å­¦ä¹ ç‡
- **`train/grad_norm`**: æ¢¯åº¦èŒƒæ•°ï¼ˆç›‘æ§æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ï¼‰
- **`train/epoch_loss`**: æ¯ä¸ª epoch çš„å¹³å‡æŸå¤±
- **`train/samples_per_second`**: è®­ç»ƒååé‡

è®­ç»ƒå¼€å§‹åï¼ŒWandB ä¼šè‡ªåŠ¨ç”Ÿæˆ URLï¼Œå¯åœ¨æµè§ˆå™¨ä¸­å®æ—¶æŸ¥çœ‹è®­ç»ƒè¿›åº¦ã€‚

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

1. **å…ˆåšå°è§„æ¨¡æµ‹è¯•**ï¼šç”¨ 10B tokens æµ‹è¯•å®é™…é€Ÿåº¦
2. **ç›‘æ§è®­ç»ƒé€Ÿåº¦**ï¼šä½¿ç”¨ WandB è®°å½• tokens/s
3. **ä¼˜åŒ–è®­ç»ƒæµç¨‹**ï¼šæ ¹æ®å®é™…é€Ÿåº¦è°ƒæ•´ batch size ç­‰å‚æ•°
4. **å‡†å¤‡å……è¶³æ—¶é—´**ï¼šé¢„ç•™ 2-3 ä¸ªæœˆæ—¶é—´çª—å£

---

**æ›´æ–°æ—¥æœŸ**: 2025-12-15
